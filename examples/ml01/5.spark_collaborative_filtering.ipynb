{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark协同过滤\n",
    "by [@寒小阳](http://blog.csdn.net/han_xiaoyang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单机版本的协同过滤很多同学都会写，当数据量大到一定程度(单机没办法全部载入)的时候，我们就要考虑用spark这种分布式的系统来处理了。我们先来看看user-based和item-based协同过滤怎么写。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user-based协同过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Could not parse Master URL: '-f'\n\tat org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2564)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:236)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5758a93c3721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"PythonUserCF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 118\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1401\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Could not parse Master URL: '-f'\n\tat org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2564)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:236)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf8 -*-\n",
    "# pySpark实现的基于用户的协同过滤\n",
    "# 使用的余弦相似度\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import random\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# user item rating timestamp\n",
    "def parseVectorOnUser(line):\n",
    "    '''\n",
    "        解析数据，key是user，后面是item和打分\n",
    "    '''\n",
    "    line = line.split(\"|\")\n",
    "    return line[0],(line[1],float(line[2]))\n",
    "\n",
    "def parseVectorOnItem(line):\n",
    "    '''\n",
    "        解析数据，key是item，后面是user和打分\n",
    "    '''\n",
    "    line = line.split(\"|\")\n",
    "    return line[1],(line[0],float(line[2]))\n",
    "\n",
    "def sampleInteractions(item_id,users_with_rating,n):\n",
    "    '''\n",
    "        如果某个商品上用户行为特别多，可以选择适当做点下采样\n",
    "    '''\n",
    "    if len(users_with_rating) > n:\n",
    "        return item_id, random.sample(users_with_rating,n)\n",
    "    else:\n",
    "        return item_id, users_with_rating\n",
    "\n",
    "def findUserPairs(item_id,users_with_rating):\n",
    "    '''\n",
    "        对每个item，找到共同打分的user对\n",
    "    '''\n",
    "    for user1,user2 in combinations(users_with_rating,2):\n",
    "        return (user1[0],user2[0]),(user1[1],user2[1])\n",
    "\n",
    "def calcSim(user_pair,rating_pairs):\n",
    "    ''' \n",
    "        对每个user对，根据打分计算余弦距离，并返回共同打分的item个数\n",
    "    '''\n",
    "    sum_xx, sum_xy, sum_yy, sum_x, sum_y, n = (0.0, 0.0, 0.0, 0.0, 0.0, 0)\n",
    "    \n",
    "    for rating_pair in rating_pairs:\n",
    "        sum_xx += np.float(rating_pair[0]) * np.float(rating_pair[0])\n",
    "        sum_yy += np.float(rating_pair[1]) * np.float(rating_pair[1])\n",
    "        sum_xy += np.float(rating_pair[0]) * np.float(rating_pair[1])\n",
    "        # sum_y += rt[1]\n",
    "        # sum_x += rt[0]\n",
    "        n += 1\n",
    "\n",
    "    cos_sim = cosine(sum_xy,np.sqrt(sum_xx),np.sqrt(sum_yy))\n",
    "    return user_pair, (cos_sim,n)\n",
    "\n",
    "def cosine(dot_product,rating_norm_squared,rating2_norm_squared):\n",
    "    '''\n",
    "        2个向量A和B的余弦相似度\n",
    "       dotProduct(A, B) / (norm(A) * norm(B))\n",
    "    '''\n",
    "    numerator = dot_product\n",
    "    denominator = rating_norm_squared * rating2_norm_squared\n",
    "\n",
    "    return (numerator / (float(denominator))) if denominator else 0.0\n",
    "\n",
    "def keyOnFirstUser(user_pair,item_sim_data):\n",
    "    '''\n",
    "        对于每个user-user对，用第一个user做key(好像有点粗暴...)\n",
    "    '''\n",
    "    (user1_id,user2_id) = user_pair\n",
    "    return user1_id,(user2_id,item_sim_data)\n",
    "\n",
    "def nearestNeighbors(user,users_and_sims,n):\n",
    "    '''\n",
    "        选出相似度最高的N个邻居\n",
    "    '''\n",
    "    users_and_sims.sort(key=lambda x: x[1][0],reverse=True)\n",
    "    return user, users_and_sims[:n]\n",
    "\n",
    "def topNRecommendations(user_id,user_sims,users_with_rating,n):\n",
    "    '''\n",
    "        根据最近的N个邻居进行推荐\n",
    "    '''\n",
    "\n",
    "    totals = defaultdict(int)\n",
    "    sim_sums = defaultdict(int)\n",
    "\n",
    "    for (neighbor,(sim,count)) in user_sims:\n",
    "\n",
    "        # 遍历邻居的打分\n",
    "        unscored_items = users_with_rating.get(neighbor,None)\n",
    "\n",
    "        if unscored_items:\n",
    "            for (item,rating) in unscored_items:\n",
    "                if neighbor != item:\n",
    "\n",
    "                    # 更新推荐度和相近度\n",
    "                    totals[neighbor] += sim * rating\n",
    "                    sim_sums[neighbor] += sim\n",
    "\n",
    "    # 归一化\n",
    "    scored_items = [(total/sim_sums[item],item) for item,total in totals.items()]\n",
    "\n",
    "    # 按照推荐度降序排列\n",
    "    scored_items.sort(reverse=True)\n",
    "\n",
    "    # 推荐度的item\n",
    "    ranked_items = [x[1] for x in scored_items]\n",
    "\n",
    "    return user_id,ranked_items[:n]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 3:\n",
    "        print >> sys.stderr, \\\n",
    "            \"Usage: PythonUserCF <master> <file>\"\n",
    "        exit(-1)\n",
    "\n",
    "    sc = SparkContext(sys.argv[1],\"PythonUserCF\")\n",
    "    lines = sc.textFile(sys.argv[2])\n",
    "\n",
    "    '''\n",
    "        处理数据，获得稀疏item-user矩阵:\n",
    "        item_id -> ((user_1,rating),(user2,rating))\n",
    "    '''\n",
    "    item_user_pairs = lines.map(parseVectorOnItem).groupByKey().map(\n",
    "        lambda p: sampleInteractions(p[0],p[1],500)).cache()\n",
    "\n",
    "    '''\n",
    "        获得2个用户所有的item-item对得分组合:\n",
    "        (user1_id,user2_id) -> [(rating1,rating2),\n",
    "                                (rating1,rating2),\n",
    "                                (rating1,rating2),\n",
    "                                ...]\n",
    "    '''\n",
    "    pairwise_users = item_user_pairs.filter(\n",
    "        lambda p: len(p[1]) > 1).map(\n",
    "        lambda p: findUserPairs(p[0],p[1])).groupByKey()\n",
    "\n",
    "    '''\n",
    "        计算余弦相似度，找到最近的N个邻居:\n",
    "        (user1,user2) ->    (similarity,co_raters_count)\n",
    "    '''\n",
    "    user_sims = pairwise_users.map(\n",
    "        lambda p: calcSim(p[0],p[1])).map(\n",
    "        lambda p: keyOnFirstUser(p[0],p[1])).groupByKey().map(\n",
    "        lambda p: nearestNeighbors(p[0],p[1],50))\n",
    "\n",
    "    ''' \n",
    "        对每个用户的打分记录整理成如下形式\n",
    "        user_id -> [(item_id_1, rating_1),\n",
    "                   [(item_id_2, rating_2),\n",
    "                    ...]\n",
    "    '''\n",
    "\n",
    "    user_item_hist = lines.map(parseVectorOnUser).groupByKey().collect()\n",
    "\n",
    "    ui_dict = {}\n",
    "    for (user,items) in user_item_hist: \n",
    "        ui_dict[user] = items\n",
    "\n",
    "    uib = sc.broadcast(ui_dict)\n",
    "\n",
    "    '''\n",
    "        为每个用户计算Top N的推荐\n",
    "        user_id -> [item1,item2,item3,...]\n",
    "    '''\n",
    "    user_item_recs = user_sims.map(lambda p: topNRecommendations(p[0],p[1],uib.value,100)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item-based协同过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding:utf8 -*-\n",
    "# pySpark实现的基于物品的协同过滤\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import pdb\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def parseVector(line):\n",
    "    '''\n",
    "        解析数据，key是item，后面是user和打分\n",
    "    '''\n",
    "    line = line.split(\"|\")\n",
    "    return line[0],(line[1],float(line[2]))\n",
    "\n",
    "def sampleInteractions(user_id,items_with_rating,n):\n",
    "    '''\n",
    "        如果某个用户打分行为特别多，可以选择适当做点下采样\n",
    "    '''\n",
    "    if len(items_with_rating) > n:\n",
    "        return user_id, random.sample(items_with_rating,n)\n",
    "    else:\n",
    "        return user_id, items_with_rating\n",
    "\n",
    "def findItemPairs(user_id,items_with_rating):\n",
    "    '''\n",
    "        对每个用户的打分item，组对\n",
    "    '''\n",
    "    for item1,item2 in combinations(items_with_rating,2):\n",
    "        return (item1[0],item2[0]),(item1[1],item2[1])\n",
    "\n",
    "def calcSim(item_pair,rating_pairs):\n",
    "    ''' \n",
    "        对每个item对，根据打分计算余弦距离，并返回共同打分的user个数\n",
    "    '''\n",
    "    sum_xx, sum_xy, sum_yy, sum_x, sum_y, n = (0.0, 0.0, 0.0, 0.0, 0.0, 0)\n",
    "    \n",
    "    for rating_pair in rating_pairs:\n",
    "        sum_xx += np.float(rating_pair[0]) * np.float(rating_pair[0])\n",
    "        sum_yy += np.float(rating_pair[1]) * np.float(rating_pair[1])\n",
    "        sum_xy += np.float(rating_pair[0]) * np.float(rating_pair[1])\n",
    "        # sum_y += rt[1]\n",
    "        # sum_x += rt[0]\n",
    "        n += 1\n",
    "\n",
    "    cos_sim = cosine(sum_xy,np.sqrt(sum_xx),np.sqrt(sum_yy))\n",
    "    return item_pair, (cos_sim,n)\n",
    "\n",
    "def cosine(dot_product,rating_norm_squared,rating2_norm_squared):\n",
    "    '''\n",
    "    The cosine between two vectors A, B\n",
    "       dotProduct(A, B) / (norm(A) * norm(B))\n",
    "    '''\n",
    "    numerator = dot_product\n",
    "    denominator = rating_norm_squared * rating2_norm_squared\n",
    "    return (numerator / (float(denominator))) if denominator else 0.0\n",
    "\n",
    "def correlation(size, dot_product, rating_sum, \\\n",
    "            rating2sum, rating_norm_squared, rating2_norm_squared):\n",
    "    '''\n",
    "        2个向量A和B的相似度\n",
    "        [n * dotProduct(A, B) - sum(A) * sum(B)] /\n",
    "        sqrt{ [n * norm(A)^2 - sum(A)^2] [n * norm(B)^2 - sum(B)^2] }\n",
    "\n",
    "    '''\n",
    "    numerator = size * dot_product - rating_sum * rating2sum\n",
    "    denominator = sqrt(size * rating_norm_squared - rating_sum * rating_sum) * \\\n",
    "                    sqrt(size * rating2_norm_squared - rating2sum * rating2sum)\n",
    "\n",
    "    return (numerator / (float(denominator))) if denominator else 0.0\n",
    "\n",
    "def keyOnFirstItem(item_pair,item_sim_data):\n",
    "    '''\n",
    "        对于每个item-item对，用第一个item做key(好像有点粗暴...)\n",
    "    '''\n",
    "    (item1_id,item2_id) = item_pair\n",
    "    return item1_id,(item2_id,item_sim_data)\n",
    "\n",
    "def nearestNeighbors(item_id,items_and_sims,n):\n",
    "    '''\n",
    "        排序选出相似度最高的N个邻居\n",
    "    '''\n",
    "    items_and_sims.sort(key=lambda x: x[1][0],reverse=True)\n",
    "    return item_id, items_and_sims[:n]\n",
    "\n",
    "def topNRecommendations(user_id,items_with_rating,item_sims,n):\n",
    "    '''\n",
    "        根据最近的N个邻居进行推荐\n",
    "    '''\n",
    "    \n",
    "    totals = defaultdict(int)\n",
    "    sim_sums = defaultdict(int)\n",
    "\n",
    "    for (item,rating) in items_with_rating:\n",
    "\n",
    "        # 遍历item的邻居\n",
    "        nearest_neighbors = item_sims.get(item,None)\n",
    "\n",
    "        if nearest_neighbors:\n",
    "            for (neighbor,(sim,count)) in nearest_neighbors:\n",
    "                if neighbor != item:\n",
    "\n",
    "                    # 更新推荐度和相近度\n",
    "                    totals[neighbor] += sim * rating\n",
    "                    sim_sums[neighbor] += sim\n",
    "\n",
    "    # 归一化\n",
    "    scored_items = [(total/sim_sums[item],item) for item,total in totals.items()]\n",
    "\n",
    "    # 按照推荐度降序排列\n",
    "    scored_items.sort(reverse=True)\n",
    "\n",
    "    ranked_items = [x[1] for x in scored_items]\n",
    "\n",
    "    return user_id,ranked_items[:n]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 3:\n",
    "        print >> sys.stderr, \\\n",
    "            \"Usage: PythonItemCF <master> <file>\"\n",
    "        exit(-1)\n",
    "\n",
    "    sc = SparkContext(sys.argv[1], \"PythonItemCF\")\n",
    "    lines = sc.textFile(sys.argv[2])\n",
    "\n",
    "    ''' \n",
    "        处理数据，获得稀疏user-item矩阵:\n",
    "        user_id -> [(item_id_1, rating_1),\n",
    "                   [(item_id_2, rating_2),\n",
    "                    ...]\n",
    "    '''\n",
    "    user_item_pairs = lines.map(parseVector).groupByKey().map(\n",
    "        lambda p: sampleInteractions(p[0],p[1],500)).cache()\n",
    "\n",
    "    '''\n",
    "        获取所有item-item组合对\n",
    "        (item1,item2) ->    [(item1_rating,item2_rating),\n",
    "                             (item1_rating,item2_rating),\n",
    "                             ...]\n",
    "    '''\n",
    "\n",
    "    pairwise_items = user_item_pairs.filter(\n",
    "        lambda p: len(p[1]) > 1).map(\n",
    "        lambda p: findItemPairs(p[0],p[1])).groupByKey()\n",
    "\n",
    "    '''\n",
    "        计算余弦相似度，找到最近的N个邻居:\n",
    "        (item1,item2) ->    (similarity,co_raters_count)\n",
    "    '''\n",
    "\n",
    "    item_sims = pairwise_items.map(\n",
    "        lambda p: calcSim(p[0],p[1])).map(\n",
    "        lambda p: keyOnFirstItem(p[0],p[1])).groupByKey().map(\n",
    "        lambda p: nearestNeighbors(p[0],p[1],50)).collect()\n",
    "\n",
    "\n",
    "    item_sim_dict = {}\n",
    "    for (item,data) in item_sims: \n",
    "        item_sim_dict[item] = data\n",
    "\n",
    "    isb = sc.broadcast(item_sim_dict)\n",
    "\n",
    "    '''\n",
    "        计算最佳的N个推荐结果\n",
    "        user_id -> [item1,item2,item3,...]\n",
    "    '''\n",
    "    user_item_recs = user_item_pairs.map(lambda p: topNRecommendations(p[0],p[1],isb.value,500)).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
